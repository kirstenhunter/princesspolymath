<!doctype html>
<html class="no-js" lang="en">
<meta http-equiv="Edge-Cache-Tag" content="princess" />
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Hadoop and AWS and Python, Oh My!</title>
	<link rel="stylesheet" type="text/css" href="https://www.princesspolymath.com/assets/css/styles_feeling_responsive.css">
	<script src="https://www.princesspolymath.com/assets/js/modernizr.min.js"></script>

	<script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
	<script>
		WebFont.load({
			google: {
				families: [ 'Lato:400,700,400italic:latin', 'Volkhov::latin' ]
			}
		});
	</script>

	<noscript>
		<link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic%7CVolkhov' rel='stylesheet' type='text/css'>
	</noscript>


	<!-- Search Engine Optimization -->
	<meta name="description" content="For an upcoming project at work, I needed to get a better idea of how the AWS services work together, and wanted to also see how the EC2 instances could be used for parallel processing.  Sadly, I do not love Java, and although I would use it if pressed, I wanted to see if I could find a pythony way to process some data using a hadoop setup.





  So, based on this page, I created a mapper and reducer in python. The mapper looks through a file and spits out lines for each match it finds.  The reducer takes the stdout from that process (using hadoop streaming) and does the thinking, then spits out the result. The examples on that page are a fine place to start for this piece.  And you can time the process on your system here to get an idea of the speedup using the hadoop setup.






  Next, I needed to get the files over to S3 so I can access them from my EC2 instance.  S3 instances are persistent, and transfers between S3 and EC2 are free, so I can run my processes an infinite number of times without incurring new costs for grabbing the files.  First, I created a bucket using the Python S3 tools, and then copied the files over using:



  
  
  
  
     
    
    hadoop fs -put &amp;lt;file&amp;gt; s3://ID:SECRET@BUCKET/name_of_dir
    
    
        
      
      
        There are, of course, other ways to move things to S3 buckets.  Pick one you like. 
        
        
        
        
        
          Now that all of my files are there for accessing, it&amp;#8217;s time to set up the hadoop instance.  
        
        
        
        
        
        
          This part isn&amp;#8217;t included in toto anywhere, so I&amp;#8217;ll cover it here in detail.  This assumes you&amp;#8217;ve done all of:
        
        
        
          
            
              Set up yourself with an AWS account with EC2 and S3 access (including setting up a properly permissioned id_rsa-gsg-keypair as described here)
            
            
              Created a bucket in S3 and populated it with files
            
            
              Created a mapper.py and reducer.py and tested them with your files
            
            
              Installed the hadoop tools on your local system and configured them as described here
            
          
        
        
        
          Next, even though every piece of documentation says to do this:
        
        
        
           
          
          bin/hadoop-ec2 run
          
          
             
            
            
            
            
            
              That&amp;#8217;s a lie.  Try this instead: 
              
              
                 
                
                bin/hadoop-ec2 launch-cluster &amp;lt;group_name&amp;gt; &amp;lt;number_of_slaves&amp;gt;
                
                
                   
                  
                  
                  
                  
                  
                    This will create a master hadoop node, and your slaves. For number_of_slaves you want to pick something &amp;lt;= 19 so that your total doesn&amp;#8217;t exceed 20 (unless you have special privileges). 
                    
                    
                    
                    
                    
                      Now we have to move our snazzy mapper and reducer to the master: 
                      
                      
                      
                      
                      
                         
                        
                        bin/hadoop-ec2-env.sh
scp $SSH_OPTS /path/to/mapper.py root@$MASTER_HOST:/home
scp $SSH_OPTS /path/to/reducer.py root@$MASTER_HOST:/home
                        
                        
                           
                          
                          
                          
                          
                          
                            &amp;#8216;run&amp;#8217; apparently used to then log you into your master, but since we&amp;#8217;re using launch-cluster, you&amp;#8217;ll need to do it yourself: 
                            
                            
                            
                            
                            
                               
                              
                              ssh $SSH_OPTS root@&amp;lt;your_new_master&amp;gt;
                              
                              
                                 
                                
                                
                                
                                
                                
                                  And there you are! On your new master. Awesome. Now let&amp;#8217;s move the data to our cluster (ID and SECRET are your AWS credentials, BUCKET is the bucket you created):
                                
                                
                                
                                   
                                  
                                  cd /usr/local/hadoop-&amp;lt;version&amp;gt;
bin/hadoop fs -mkdir files
bin/hadoop distcp s3://&amp;lt;ID&amp;gt;:&amp;lt;SECRET&amp;gt;@&amp;lt;BUCKET&amp;gt;/path/to/files files
                                  
                                  
                                     
                                    
                                    
                                    
                                    
                                    
                                      Ok, great. Almost there. Now we need to run the thing: 
                                      
                                      
                                        
                                           
                                          
                                          hadoop@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-0.18.0-streaming.jar -mapper mapper.py -file /home/mapper.py -reducer reducer.py -file /home/reducer.py -input files/* -output map-reduce.output
                                          
                                          
                                              
                                            
                                            
                                              While it&amp;#8217;s running, you can check out the neat web report hadoop creates at http://:50030.  Go ahead, check it out.  It&amp;#8217;s totally cool.">
  	<meta name="google-site-verification" content="Vk0IOJ2jwG_qEoG7fuEXYqv0m2rLa8P778Fi_GrsgEQ">
	<meta name="msvalidate.01" content="0FB4C028ABCF07C908C54386ABD2D97F" >
	<link rel="author" href="https://plus.google.com/u/0/118311555303973066167">
	
	


	<!-- Facebook Open Graph -->
	<meta property="og:title" content="Hadoop and AWS and Python, Oh My!">
	<meta property="og:description" content="For an upcoming project at work, I needed to get a better idea of how the AWS services work together, and wanted to also see how the EC2 instances could be used for parallel processing.  Sadly, I do not love Java, and although I would use it if pressed, I wanted to see if I could find a pythony way to process some data using a hadoop setup.





  So, based on this page, I created a mapper and reducer in python. The mapper looks through a file and spits out lines for each match it finds.  The reducer takes the stdout from that process (using hadoop streaming) and does the thinking, then spits out the result. The examples on that page are a fine place to start for this piece.  And you can time the process on your system here to get an idea of the speedup using the hadoop setup.






  Next, I needed to get the files over to S3 so I can access them from my EC2 instance.  S3 instances are persistent, and transfers between S3 and EC2 are free, so I can run my processes an infinite number of times without incurring new costs for grabbing the files.  First, I created a bucket using the Python S3 tools, and then copied the files over using:



  
  
  
  
     
    
    hadoop fs -put &amp;lt;file&amp;gt; s3://ID:SECRET@BUCKET/name_of_dir
    
    
        
      
      
        There are, of course, other ways to move things to S3 buckets.  Pick one you like. 
        
        
        
        
        
          Now that all of my files are there for accessing, it&amp;#8217;s time to set up the hadoop instance.  
        
        
        
        
        
        
          This part isn&amp;#8217;t included in toto anywhere, so I&amp;#8217;ll cover it here in detail.  This assumes you&amp;#8217;ve done all of:
        
        
        
          
            
              Set up yourself with an AWS account with EC2 and S3 access (including setting up a properly permissioned id_rsa-gsg-keypair as described here)
            
            
              Created a bucket in S3 and populated it with files
            
            
              Created a mapper.py and reducer.py and tested them with your files
            
            
              Installed the hadoop tools on your local system and configured them as described here
            
          
        
        
        
          Next, even though every piece of documentation says to do this:
        
        
        
           
          
          bin/hadoop-ec2 run
          
          
             
            
            
            
            
            
              That&amp;#8217;s a lie.  Try this instead: 
              
              
                 
                
                bin/hadoop-ec2 launch-cluster &amp;lt;group_name&amp;gt; &amp;lt;number_of_slaves&amp;gt;
                
                
                   
                  
                  
                  
                  
                  
                    This will create a master hadoop node, and your slaves. For number_of_slaves you want to pick something &amp;lt;= 19 so that your total doesn&amp;#8217;t exceed 20 (unless you have special privileges). 
                    
                    
                    
                    
                    
                      Now we have to move our snazzy mapper and reducer to the master: 
                      
                      
                      
                      
                      
                         
                        
                        bin/hadoop-ec2-env.sh
scp $SSH_OPTS /path/to/mapper.py root@$MASTER_HOST:/home
scp $SSH_OPTS /path/to/reducer.py root@$MASTER_HOST:/home
                        
                        
                           
                          
                          
                          
                          
                          
                            &amp;#8216;run&amp;#8217; apparently used to then log you into your master, but since we&amp;#8217;re using launch-cluster, you&amp;#8217;ll need to do it yourself: 
                            
                            
                            
                            
                            
                               
                              
                              ssh $SSH_OPTS root@&amp;lt;your_new_master&amp;gt;
                              
                              
                                 
                                
                                
                                
                                
                                
                                  And there you are! On your new master. Awesome. Now let&amp;#8217;s move the data to our cluster (ID and SECRET are your AWS credentials, BUCKET is the bucket you created):
                                
                                
                                
                                   
                                  
                                  cd /usr/local/hadoop-&amp;lt;version&amp;gt;
bin/hadoop fs -mkdir files
bin/hadoop distcp s3://&amp;lt;ID&amp;gt;:&amp;lt;SECRET&amp;gt;@&amp;lt;BUCKET&amp;gt;/path/to/files files
                                  
                                  
                                     
                                    
                                    
                                    
                                    
                                    
                                      Ok, great. Almost there. Now we need to run the thing: 
                                      
                                      
                                        
                                           
                                          
                                          hadoop@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-0.18.0-streaming.jar -mapper mapper.py -file /home/mapper.py -reducer reducer.py -file /home/reducer.py -input files/* -output map-reduce.output
                                          
                                          
                                              
                                            
                                            
                                              While it&amp;#8217;s running, you can check out the neat web report hadoop creates at http://:50030.  Go ahead, check it out.  It&amp;#8217;s totally cool.">
	<meta property="og:url" content="https://www.princesspolymath.com/hadoop-and-aws-and-python-oh-my.html">
	<meta property="og:locale" content="en_EN">
	<meta property="og:type" content="website">
	<meta property="og:site_name" content="Princess Polymath">
	
	<meta property="article:author" content="https://www.facebook.com/princesspolymath">


	
	<!-- Twitter -->
	<meta name="twitter:card" content="summary">
	<meta name="twitter:site" content="synedra">
	<meta name="twitter:creator" content="synedra">
	<meta name="twitter:title" content="Hadoop and AWS and Python, Oh My!">
	<meta name="twitter:description" content="For an upcoming project at work, I needed to get a better idea of how the AWS services work together, and wanted to also see how the EC2 instances could be used for parallel processing.  Sadly, I do not love Java, and although I would use it if pressed, I wanted to see if I could find a pythony way to process some data using a hadoop setup.





  So, based on this page, I created a mapper and reducer in python. The mapper looks through a file and spits out lines for each match it finds.  The reducer takes the stdout from that process (using hadoop streaming) and does the thinking, then spits out the result. The examples on that page are a fine place to start for this piece.  And you can time the process on your system here to get an idea of the speedup using the hadoop setup.






  Next, I needed to get the files over to S3 so I can access them from my EC2 instance.  S3 instances are persistent, and transfers between S3 and EC2 are free, so I can run my processes an infinite number of times without incurring new costs for grabbing the files.  First, I created a bucket using the Python S3 tools, and then copied the files over using:



  
  
  
  
     
    
    hadoop fs -put &amp;lt;file&amp;gt; s3://ID:SECRET@BUCKET/name_of_dir
    
    
        
      
      
        There are, of course, other ways to move things to S3 buckets.  Pick one you like. 
        
        
        
        
        
          Now that all of my files are there for accessing, it&amp;#8217;s time to set up the hadoop instance.  
        
        
        
        
        
        
          This part isn&amp;#8217;t included in toto anywhere, so I&amp;#8217;ll cover it here in detail.  This assumes you&amp;#8217;ve done all of:
        
        
        
          
            
              Set up yourself with an AWS account with EC2 and S3 access (including setting up a properly permissioned id_rsa-gsg-keypair as described here)
            
            
              Created a bucket in S3 and populated it with files
            
            
              Created a mapper.py and reducer.py and tested them with your files
            
            
              Installed the hadoop tools on your local system and configured them as described here
            
          
        
        
        
          Next, even though every piece of documentation says to do this:
        
        
        
           
          
          bin/hadoop-ec2 run
          
          
             
            
            
            
            
            
              That&amp;#8217;s a lie.  Try this instead: 
              
              
                 
                
                bin/hadoop-ec2 launch-cluster &amp;lt;group_name&amp;gt; &amp;lt;number_of_slaves&amp;gt;
                
                
                   
                  
                  
                  
                  
                  
                    This will create a master hadoop node, and your slaves. For number_of_slaves you want to pick something &amp;lt;= 19 so that your total doesn&amp;#8217;t exceed 20 (unless you have special privileges). 
                    
                    
                    
                    
                    
                      Now we have to move our snazzy mapper and reducer to the master: 
                      
                      
                      
                      
                      
                         
                        
                        bin/hadoop-ec2-env.sh
scp $SSH_OPTS /path/to/mapper.py root@$MASTER_HOST:/home
scp $SSH_OPTS /path/to/reducer.py root@$MASTER_HOST:/home
                        
                        
                           
                          
                          
                          
                          
                          
                            &amp;#8216;run&amp;#8217; apparently used to then log you into your master, but since we&amp;#8217;re using launch-cluster, you&amp;#8217;ll need to do it yourself: 
                            
                            
                            
                            
                            
                               
                              
                              ssh $SSH_OPTS root@&amp;lt;your_new_master&amp;gt;
                              
                              
                                 
                                
                                
                                
                                
                                
                                  And there you are! On your new master. Awesome. Now let&amp;#8217;s move the data to our cluster (ID and SECRET are your AWS credentials, BUCKET is the bucket you created):
                                
                                
                                
                                   
                                  
                                  cd /usr/local/hadoop-&amp;lt;version&amp;gt;
bin/hadoop fs -mkdir files
bin/hadoop distcp s3://&amp;lt;ID&amp;gt;:&amp;lt;SECRET&amp;gt;@&amp;lt;BUCKET&amp;gt;/path/to/files files
                                  
                                  
                                     
                                    
                                    
                                    
                                    
                                    
                                      Ok, great. Almost there. Now we need to run the thing: 
                                      
                                      
                                        
                                           
                                          
                                          hadoop@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-0.18.0-streaming.jar -mapper mapper.py -file /home/mapper.py -reducer reducer.py -file /home/reducer.py -input files/* -output map-reduce.output
                                          
                                          
                                              
                                            
                                            
                                              While it&amp;#8217;s running, you can check out the neat web report hadoop creates at http://:50030.  Go ahead, check it out.  It&amp;#8217;s totally cool.">
	
	

	<link type="text/plain" rel="author" href="https://www.princesspolymath.com/humans.txt">

	

	

	<link rel="icon" sizes="32x32" href="https://www.princesspolymath.com/assets/img/favicon-32x32.png">

	<link rel="icon" sizes="192x192" href="https://www.princesspolymath.com/assets/img/touch-icon-192x192.png">

	<link rel="apple-touch-icon-precomposed" sizes="180x180" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-180x180-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="152x152" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-152x152-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-144x144-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="120x120" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-120x120-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-114x114-precomposed.png">

	
	<link rel="apple-touch-icon-precomposed" sizes="76x76" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-76x76-precomposed.png">

	<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-72x72-precomposed.png">

	<link rel="apple-touch-icon-precomposed" href="https://www.princesspolymath.com/assets/img/apple-touch-icon-precomposed.png">	

	<meta name="msapplication-TileImage" content="https://www.princesspolymath.com/assets/img/msapplication_tileimage.png">

	<meta name="msapplication-TileColor" content="#fabb00">


	

</head>
<body id="top-of-page" class="">
	
	<div id="navigation" class="sticky">
  <nav class="top-bar" role="navigation" data-topbar>
    <ul class="title-area">
      <li class="name">
      <h1 class="show-for-small-only"><a href="https://www.princesspolymath.com" class="icon-tree"> Princess Polymath</a></h1>
    </li>
       <!-- Remove the class "menu-icon" to get rid of menu icon. Take out "Menu" to just have icon alone -->
      <li class="toggle-topbar menu-icon"><a href="#"><span>Navigation</span></a></li>
    </ul>
    <section class="top-bar-section">

      <ul class="right">
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
        

              

          
          
            
            
              <li class="divider"></li>
              <li><a href="https://www.princesspolymath.com/search/">Search</a></li>

            
            
          
        

              

          
          
            
            
              <li class="divider"></li>
              <li><a href="https://www.princesspolymath.com/contact/">Contact</a></li>

            
            
          
        
        
      </ul>

      <ul class="left">
        

              

          
          

            
            
	    
		<li><a href="https://www.princesspolymath.com/">Home</a></li>
              <li class="divider"></li>
		
            
            
          
        

              

          
          

            
            
	    
		<li><a href="https://www.princesspolymath.com/resume">Resume</a></li>
              <li class="divider"></li>
		
            
            
          
        

              

          
          

            
            
	    
		<li><a href="http://github.com/synedra" target="_blank">CODE</a></li>
              <li class="divider"></li>
		
            
            
          
        

              

          
          

            
            
	    
		<li><a href="http://irresistibleapis.com" target="_blank">Irresistible APIs Book</a></li>
              <li class="divider"></li>
		
            
            
          
        

              

          
          

            
            
	    
		<li><a href="https://www.princesspolymath.com/schedule">Schedule</a></li>
              <li class="divider"></li>
		
            
            
          
        

              

          
          

            
            
	    
		<li><a href="https://www.princesspolymath.com/presentations">Presentations</a></li>
              <li class="divider"></li>
		
            
            
          
        

              

          
          

            
            
	    
		<li><a href="https://www.princesspolymath.com/reviews">Reviews</a></li>
              <li class="divider"></li>
		
            
            
          
        

              

          
          

            
            

              <li class="has-dropdown">
                <a href="https://www.princesspolymath.com/blog/">Blog</a>

                  <ul class="dropdown">
                    

                      

                      <li><a href="https://www.princesspolymath.com/blog/archive/">Blog Archive</a></li>
                    
                  </ul>

              </li>
              <li class="divider"></li>
            
          
        

              

          
          
        

              

          
          
        
        
      </ul>
    </section>
  </nav>
</div><!-- /#navigation -->

	
	
	

<div id="masthead-no-image-header">
	<div class="row">
		<div class="small-12 columns">
			<a id="logo" href="https://www.princesspolymath.com" title="Princess Polymath – I know lots of things...">
				<img src="https://www.princesspolymath.com/assets/img/logo-words.png" alt="Princess Polymath – I know lots of things...">
			</a>
		</div><!-- /.small-12.columns -->
	</div><!-- /.row -->
</div><!-- /#masthead -->









	

	<div class="row t30">
	<div class="medium-8 columns medium-push-4">
		<article itemscope itemtype="http://schema.org/Article">
			<header>
				

				<div itemprop="name">
					
					<h1>Hadoop and AWS and Python, Oh My!</h1>
				</div>
			</header>


			

			<div itemprop="articleSection">
			<p>For an upcoming project at work, I needed to get a better idea of how the <a href="http://aws.amazon.com/">AWS</a> services work together, and wanted to also see how the EC2 instances could be used for parallel processing.  Sadly, I do not love Java, and although I would use it if pressed, I wanted to see if I could find a pythony way to process some data using a hadoop setup.</p>

<div>
</div>

<div>
  So, based on <a href="http://www.michael-noll.com/wiki/Writing_An_Hadoop_MapReduce_Program_In_Python">this page</a>, I created a mapper and reducer in python. The mapper looks through a file and spits out lines for each match it finds.  The reducer takes the stdout from that process (using hadoop streaming) and does the thinking, then spits out the result. The examples on that page are a fine place to start for this piece.  And you can time the process on your system here to get an idea of the speedup using the hadoop setup.
</div>

<div>
</div>

<div>
  Next, I needed to get the files over to S3 so I can access them from my EC2 instance.  S3 instances are persistent, and transfers between S3 and EC2 are free, so I can run my processes an infinite number of times without incurring new costs for grabbing the files.  First, I created a bucket using the Python S3 tools, and then copied the files over using:
</div>

<div>
  <div>
  </div>
  
  <div>
    <span class="Apple-style-span" style="color: rgb(0, 0, 0); font-family: verdana; font-size: 12px; -webkit-border-horizontal-spacing: 3px; -webkit-border-vertical-spacing: 3px; "> 
    
    <pre style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'Courier New', Courier, mono; font-size: 12px; background-color: rgb(239, 247, 255); border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: dashed; border-right-style: dashed; border-bottom-style: dashed; border-left-style: dashed; border-top-color: rgb(51, 51, 51); border-right-color: rgb(51, 51, 51); border-bottom-color: rgb(51, 51, 51); border-left-color: rgb(51, 51, 51); overflow-x: auto; overflow-y: auto; width: 600px; padding-top: 15px; padding-right: 10px; padding-bottom: 15px; padding-left: 10px; "><span class="Apple-style-span" style="color: rgb(51, 51, 51); font-family: arial; font-size: 13px; white-space: normal; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; "><code>hadoop fs -put &lt;file&gt; s3://ID:SECRET@BUCKET/name_of_dir</code></span></pre>
    
    <p>
        
      
      <p>
        There are, of course, other ways to move things to S3 buckets.  Pick one you like. 
        
        <div>
        </div>
        
        <div>
          Now that all of my files are there for accessing, it&#8217;s time to set up the hadoop instance.  
        </div>
        
        <div>
        </div>
        
        <div>
          This part isn&#8217;t included in toto anywhere, so I&#8217;ll cover it here in detail.  This assumes you&#8217;ve done all of:
        </div>
        
        <div>
          <ul style="border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 1em; font-weight: normal; margin-top: 0px; margin-right: 0px; margin-bottom: 0.75em; margin-left: 20px; background-repeat: repeat-y; list-style-type: disc; list-style-position: outside; list-style-image: initial; ">
            <li style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 1em; font-weight: normal; ">
              Set up yourself with an AWS account with EC2 and S3 access (including setting up a properly permissioned id_rsa-gsg-keypair as described <a href="http://docs.amazonwebservices.com/AWSEC2/2007-08-29/GettingStartedGuide/running-an-instance.html" style="text-decoration: underline; ">here</a>)
            </li>
            <li style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 1em; font-weight: normal; ">
              Created a bucket in S3 and populated it with files
            </li>
            <li style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 1em; font-weight: normal; ">
              Created a mapper.py and reducer.py and tested them with your files
            </li>
            <li style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; border-top-width: 0px; border-right-width: 0px; border-bottom-width: 0px; border-left-width: 0px; border-style: initial; border-color: initial; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 1em; font-weight: normal; ">
              Installed the hadoop tools on your local system and configured them as described <a href="http://wiki.apache.org/hadoop/AmazonEC2" style="text-decoration: underline; ">here</a>
            </li>
          </ul>
        </div>
        
        <div>
          Next, even though every piece of documentation says to do this:
        </div>
        
        <div>
          <span class="Apple-style-span" style="color: rgb(0, 0, 0); font-family: verdana; font-size: 12px; -webkit-border-horizontal-spacing: 3px; -webkit-border-vertical-spacing: 3px; "> 
          
          <pre style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'Courier New', Courier, mono; font-size: 12px; background-color: rgb(239, 247, 255); border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: dashed; border-right-style: dashed; border-bottom-style: dashed; border-left-style: dashed; border-top-color: rgb(51, 51, 51); border-right-color: rgb(51, 51, 51); border-bottom-color: rgb(51, 51, 51); border-left-color: rgb(51, 51, 51); overflow-x: auto; overflow-y: auto; width: 600px; padding-top: 15px; padding-right: 10px; padding-bottom: 15px; padding-left: 10px; ">bin/hadoop-ec2 run</pre>
          
          <p>
             
            
            <div>
            </div>
            
            <p>
              That&#8217;s a lie.  Try this instead: 
              
              <div>
                <span class="Apple-style-span" style="color: rgb(0, 0, 0); font-family: verdana; font-size: 12px; -webkit-border-horizontal-spacing: 3px; -webkit-border-vertical-spacing: 3px; "> 
                
                <pre style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'Courier New', Courier, mono; font-size: 12px; background-color: rgb(239, 247, 255); border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: dashed; border-right-style: dashed; border-bottom-style: dashed; border-left-style: dashed; border-top-color: rgb(51, 51, 51); border-right-color: rgb(51, 51, 51); border-bottom-color: rgb(51, 51, 51); border-left-color: rgb(51, 51, 51); overflow-x: auto; overflow-y: auto; width: 600px; padding-top: 15px; padding-right: 10px; padding-bottom: 15px; padding-left: 10px; ">bin/hadoop-ec2 launch-cluster &lt;group_name&gt; &lt;number_of_slaves&gt;</pre>
                
                <p>
                   
                  
                  <div>
                  </div>
                  
                  <p>
                    This will create a master hadoop node, and your slaves. For number_of_slaves you want to pick something &lt;= 19 so that your total doesn&#8217;t exceed 20 (unless you have special privileges). 
                    
                    <div>
                    </div>
                    
                    <p>
                      Now we have to move our snazzy mapper and reducer to the master: 
                      
                      <div>
                      </div>
                      
                      <div>
                        <span class="Apple-style-span" style="color: rgb(0, 0, 0); font-family: verdana; font-size: 12px; -webkit-border-horizontal-spacing: 3px; -webkit-border-vertical-spacing: 3px; "> 
                        
                        <pre style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'Courier New', Courier, mono; font-size: 12px; background-color: rgb(239, 247, 255); border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: dashed; border-right-style: dashed; border-bottom-style: dashed; border-left-style: dashed; border-top-color: rgb(51, 51, 51); border-right-color: rgb(51, 51, 51); border-bottom-color: rgb(51, 51, 51); border-left-color: rgb(51, 51, 51); overflow-x: auto; overflow-y: auto; width: 600px; padding-top: 15px; padding-right: 10px; padding-bottom: 15px; padding-left: 10px; ">bin/hadoop-ec2-env.sh
scp $SSH_OPTS /path/to/mapper.py root@$MASTER_HOST:/home
scp $SSH_OPTS /path/to/reducer.py root@$MASTER_HOST:/home</pre>
                        
                        <p>
                           
                          
                          <div>
                          </div>
                          
                          <p>
                            &#8216;run&#8217; apparently used to then log you into your master, but since we&#8217;re using launch-cluster, you&#8217;ll need to do it yourself: 
                            
                            <div>
                            </div>
                            
                            <div>
                              <span class="Apple-style-span" style="color: rgb(0, 0, 0); font-family: verdana; font-size: 12px; -webkit-border-horizontal-spacing: 3px; -webkit-border-vertical-spacing: 3px; "> 
                              
                              <pre style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'Courier New', Courier, mono; font-size: 12px; background-color: rgb(239, 247, 255); border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: dashed; border-right-style: dashed; border-bottom-style: dashed; border-left-style: dashed; border-top-color: rgb(51, 51, 51); border-right-color: rgb(51, 51, 51); border-bottom-color: rgb(51, 51, 51); border-left-color: rgb(51, 51, 51); overflow-x: auto; overflow-y: auto; width: 600px; padding-top: 15px; padding-right: 10px; padding-bottom: 15px; padding-left: 10px; ">ssh $SSH_OPTS root@&lt;your_new_master&gt;</pre>
                              
                              <p>
                                 
                                
                                <div>
                                </div>
                                
                                <p>
                                  And there you are! On your new master. Awesome. Now let&#8217;s move the data to our cluster (ID and SECRET are your AWS credentials, BUCKET is the bucket you created):
                                </p>
                                
                                <div>
                                  <span class="Apple-style-span" style="color: rgb(0, 0, 0); font-family: verdana; font-size: 12px; -webkit-border-horizontal-spacing: 3px; -webkit-border-vertical-spacing: 3px; "> 
                                  
                                  <pre style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'Courier New', Courier, mono; font-size: 12px; background-color: rgb(239, 247, 255); border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: dashed; border-right-style: dashed; border-bottom-style: dashed; border-left-style: dashed; border-top-color: rgb(51, 51, 51); border-right-color: rgb(51, 51, 51); border-bottom-color: rgb(51, 51, 51); border-left-color: rgb(51, 51, 51); overflow-x: auto; overflow-y: auto; width: 600px; padding-top: 15px; padding-right: 10px; padding-bottom: 15px; padding-left: 10px; ">cd /usr/local/hadoop-<em>&lt;version&gt;</em>
bin/hadoop fs -mkdir files
bin/hadoop distcp s3://<em>&lt;ID&gt;</em>:<em>&lt;SECRET&gt;</em>@<em>&lt;BUCKET&gt;</em>/path/to/files files</pre>
                                  
                                  <p>
                                     
                                    
                                    <div>
                                    </div>
                                    
                                    <p>
                                      Ok, great. Almost there. Now we need to run the thing: 
                                      
                                      <div>
                                        <div>
                                          <span class="Apple-style-span" style="color: rgb(0, 0, 0); font-family: verdana; font-size: 12px; -webkit-border-horizontal-spacing: 3px; -webkit-border-vertical-spacing: 3px; "> 
                                          
                                          <pre style="margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font-family: 'Courier New', Courier, mono; font-size: 12px; background-color: rgb(239, 247, 255); border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-top-style: dashed; border-right-style: dashed; border-bottom-style: dashed; border-left-style: dashed; border-top-color: rgb(51, 51, 51); border-right-color: rgb(51, 51, 51); border-bottom-color: rgb(51, 51, 51); border-left-color: rgb(51, 51, 51); overflow-x: auto; overflow-y: auto; width: 600px; padding-top: 15px; padding-right: 10px; padding-bottom: 15px; padding-left: 10px; ">hadoop@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-0.18.0-streaming.jar -mapper mapper.py -file /home/mapper.py -reducer reducer.py -file /home/reducer.py -input files/* -output map-reduce.output</pre>
                                          
                                          <p>
                                              
                                            
                                            <p>
                                              While it&#8217;s running, you can check out the neat web report hadoop creates at http://<server_name>:50030.  Go ahead, check it out.  It&#8217;s totally cool.
                                            
</server_name></p></p></span></div></div></p></p></span></div></p></span></div></p></p></span></div></p></p></p></span></div></p></p></span></div></p></p></span></div></div>

			</div>

			
						<div id="page-meta" class="t30">
				<p>
					<!-- Look the author details up from the site config. -->
					
					<!-- Output author details if some exist. -->
					

				
				<time class="icon-calendar pr20" datetime="2008-10-30" itemprop="datePublished"> 2008-10-30</time>
				

				<span class="icon-archive pr20"> UNCATEGORIZED</span>
				<br />
				<span class="pr20"><span class="icon-price-tag pr10"> aws</span> <span class="icon-price-tag pr10"> hadoop</span> <span class="icon-price-tag pr10"> python</span> </span>
			</p>

			<div id="post-nav" class="row">
				
				<div class="small-5 columns"><a class="button small radius prev" href="https://www.princesspolymath.com/hacking-on-freebase.html">&laquo; Hacking on Freebase</a></div><!-- /.small-4.columns -->
				
				<div class="small-2 columns text-center"><a class="radius button small" href="https://www.princesspolymath.com/blog/archive/" title="Blog Archive">Archive</a></div><!-- /.small-4.columns -->
				
				<div class="small-5 columns text-right"><a class="button small radius next" href="https://www.princesspolymath.com/twitter-badge-with-location.html">Twitter badge with location &raquo;</a></div><!-- /.small-4.columns -->
				
			</div>
			</div><!--  /.page-meta -->
			

			
						
				<h3 id="comments" class="t60">Dialogue &amp; Discussion</h3>
			    <div id="disqus_thread"></div>
			    <script type="text/javascript">
			        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
			        var disqus_shortname = 'princesspolymath'; 
			        var disqus_identifier = '/hadoop-and-aws-and-python-oh-my.html';

			        /* * * DON'T EDIT BELOW THIS LINE * * */
			        (function() {
			            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
			        })();
			    </script>
			    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
			



			
		</article>
	</div><!-- /.medium-8.columns -->


	
	<div class="medium-4 columns medium-pull-8">
		<aside>
<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    	<img src="http://princesspolymath.com/assets/img/headshot.jpg" alt="Kirsten Hunter">
  </div>

  <div class="author__content">
    <h3 class="author__name">Kirsten Hunter</h3>
    <p class="author__bio">I know lots of things...</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls ">
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Santa Cruz</li>
        <li><a href="mailto:synedra@gmail.com"><i class="fa fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        <li><a href="https://twitter.com/@synedra"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        <li><a href="https://www.linkedin.com/in/http://linkedin.com/in/synedra"><i class="fa fa-fw fa-linkedin-square" aria-hidden="true"></i> LinkedIn</a></li>
        <li><a href="https://github.com/https://github.com/synedra"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
    </ul>
  </div>
<a class="twitter-timeline" href="https://twitter.com/search?q=%40synedra" data-widget-id="744007280098574337">Tweets about @synedra</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

<a class="twitter-timeline" href="https://twitter.com/synedra">Tweets by synedra</a>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</aside>

	</div><!-- /.medium-4.columns -->
	


	
</div><!-- /.row -->


	
	    <div id="up-to-top" class="row">
      <div class="small-12 columns" style="text-align: right;">
        <a class="iconfont" href="#top-of-page">&#xf108;</a>
      </div><!-- /.small-12.columns -->
    </div><!-- /.row -->


    <footer id="footer-content" class="bg-grau">
      <div id="footer">
        <div class="row">
          <div class="medium-6 large-5 columns">
            <h5 class="shadow-black">About This Site</h5>

            <p class="shadow-black">
              Princess Polymath is a website focused on programming, web design, APIs and general amusement.
              <a href="https://www.princesspolymath.com/info/">More ›</a>
            </p>
          </div><!-- /.large-6.columns -->


          <div class="small-6 medium-3 large-3 large-offset-1 columns">
            
              
                <h5 class="shadow-black">Services</h5>
              
            
              
            
              
            
              
            
              
            

              <ul class="no-bullet shadow-black">
              
                
                  <li >
                    <a href="https://www.princesspolymath.com"  title=""></a>
                  </li>
              
                
                  <li >
                    <a href="https://www.princesspolymath.com/contact/"  title="Contact">Contact</a>
                  </li>
              
                
                  <li >
                    <a href="https://www.princesspolymath.com/feed.xml"  title="Subscribe to RSS Feed">RSS</a>
                  </li>
              
                
                  <li >
                    <a href="https://www.princesspolymath.com/atom.xml"  title="Subscribe to Atom Feed">Atom</a>
                  </li>
              
                
                  <li >
                    <a href="https://www.princesspolymath.com/sitemap.xml"  title="Sitemap for Google Webmaster Tools">sitemap.xml</a>
                  </li>
              
              </ul>
          </div><!-- /.large-4.columns -->


          <div class="small-6 medium-3 large-3 columns">
            
              
                <h5 class="shadow-black">Contact information</h5>
              
            
              
            
              
            

            <ul class="no-bullet shadow-black">
            
              
                <li >
                  <a href="https://www.princesspolymath.com"  title=""></a>
                </li>
            
              
                <li class="network-entypo" >
                  <a href="http://linkedin.com/in/synedra" target="_blank"  title="LinkedIn Contact">LinkedIn</a>
                </li>
            
              
                <li class="network-entypo" >
                  <a href="http://twitter.com/synedra" target="_blank"  title="Twitter">Twitter</a>
                </li>
            
            </ul>
          </div><!-- /.large-3.columns -->
        </div><!-- /.row -->

      </div><!-- /#footer -->


      <div id="subfooter">
        <nav class="row">
          <section id="subfooter-left" class="small-12 medium-6 columns credits">
            <p>Created with &hearts; by <a href="/">synedra</a> with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> based on <a href="http://phlow.github.io/feeling-responsive/">Feeling Responsive</a>.</p>
          </section>

          <section id="subfooter-right" class="small-12 medium-6 columns">
            <ul class="inline-list social-icons">
            
              <li><a href="http://github.com/synedra" target="_blank" class="icon-github" title="Riff Raff"></a></li>
            
              <li><a href="http://www.youtube.com/PhlowMedia" target="_blank" class="icon-youtube" title="Videos, Video-Anleitungen und Filme von Phlow auf YouTube"></a></li>
            
              <li><a href="http://twitter.com/synedra" target="_blank" class="icon-twitter" title="My 140 Character Babblings"></a></li>
            
              <li><a href="https://plus.google.com/u/0/+Phlow" target="_blank" class="icon-googleplus" title="YouTube Google+"></a></li>
            
            </ul>
          </section>
        </nav>
      </div><!-- /#subfooter -->
    </footer>

	

	


<script src="https://www.princesspolymath.com/assets/js/javascript.min.js"></script>







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60112281-1', 'auto');
  ga('set', 'anonymizeIp', true);
  ga('send', 'pageview');

</script>








</body>
</html>

